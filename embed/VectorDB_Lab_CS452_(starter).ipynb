{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jjwjohns/CS452/blob/main/embed/VectorDB_Lab_CS452_(starter).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4UOUNsUTsvcn"
      },
      "outputs": [],
      "source": [
        "# Download datasets from kaggle\n",
        "\n",
        "import json\n",
        "import os\n",
        "\n",
        "if not os.path.exists(\"lex-fridman-text-embedding-3-large-128.zip\"):\n",
        "  kaggle_json = {\"username\": \"michaeltreynolds\",\"key\": \"149701be742f30a8a0526762c61beea0\"}\n",
        "  kaggle_dir = os.path.join(os.path.expanduser(\"~\"), \".kaggle\")\n",
        "  os.makedirs(kaggle_dir, exist_ok=True)\n",
        "  kaggle_config_path = os.path.join(kaggle_dir, \"kaggle.json\")\n",
        "  with open(kaggle_config_path, 'w') as f:\n",
        "    json.dump(kaggle_json, f)\n",
        "\n",
        "  !kaggle datasets download -d michaeltreynolds/lex-fridman-text-embedding-3-large-128\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip kaggle data\n",
        "\n",
        "!unzip lex-fridman-text-embedding-3-large-128.zip\n",
        "!unzip lex-fridman-text-embedding-3-large-128/*.zip\n"
      ],
      "metadata": {
        "id": "h3swnD70x4FG",
        "outputId": "f2df8d65-8801-4afb-990a-d77276526abf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  lex-fridman-text-embedding-3-large-128.zip\n",
            "replace documents/documents/batch_request_0lw3vrQqdWbdBRurTGNMHU76.jsonl? [y]es, [n]o, [A]ll, [N]one, [r]ename: unzip:  cannot find or open lex-fridman-text-embedding-3-large-128/*.zip, lex-fridman-text-embedding-3-large-128/*.zip.zip or lex-fridman-text-embedding-3-large-128/*.zip.ZIP.\n",
            "\n",
            "No zipfiles found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use specific libraries\n",
        "!pip install datasets==2.20.0 psycopg2==2.9.9 pgcopy==1.6.0\n",
        "import psycopg2"
      ],
      "metadata": {
        "id": "SYDFzWfv4HLW",
        "outputId": "c8c18525-8ca1-484a-ac3b-d2c2f1552181",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets==2.20.0 in /usr/local/lib/python3.12/dist-packages (2.20.0)\n",
            "Requirement already satisfied: psycopg2==2.9.9 in /usr/local/lib/python3.12/dist-packages (2.9.9)\n",
            "Requirement already satisfied: pgcopy==1.6.0 in /usr/local/lib/python3.12/dist-packages (1.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets==2.20.0) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets==2.20.0) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets==2.20.0) (18.1.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.12/dist-packages (from datasets==2.20.0) (0.7)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets==2.20.0) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets==2.20.0) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets==2.20.0) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets==2.20.0) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets==2.20.0) (3.6.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from datasets==2.20.0) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets==2.20.0) (2024.5.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from datasets==2.20.0) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.12/dist-packages (from datasets==2.20.0) (0.35.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets==2.20.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets==2.20.0) (6.0.3)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.12/dist-packages (from pgcopy==1.6.0) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.20.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.20.0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.20.0) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.20.0) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.20.0) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.20.0) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.20.0) (1.22.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.21.2->datasets==2.20.0) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.21.2->datasets==2.20.0) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==2.20.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==2.20.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==2.20.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==2.20.0) (2025.10.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==2.20.0) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==2.20.0) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==2.20.0) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get your own trial account at timescaledb and paste your own connection string\n",
        "\n",
        "#TODO\n",
        "CONNECTION = \"postgres://tsdbadmin:aqtd6xjf37fdg2fw@avhfgf31h1.veboosbd04.tsdb.cloud.timescale.com:35070/tsdb?sslmode=require\""
      ],
      "metadata": {
        "id": "ukT4dY-z25XG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use this if you want to start over on your postgres table!\n",
        "\n",
        "DROP_TABLE = \"DROP TABLE IF EXISTS podcast, segment\"\n",
        "with psycopg2.connect(CONNECTION) as conn:\n",
        "    cursor = conn.cursor()\n",
        "    cursor.execute(DROP_TABLE)\n",
        "    conn.commit() # Commit the changes\n"
      ],
      "metadata": {
        "id": "gpp_3EuU3SN-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Useful function that takes a pd.DataFrame and copies it directly into a table.\n",
        "\n",
        "import pandas as pd\n",
        "import io\n",
        "import psycopg2\n",
        "\n",
        "from typing import List\n",
        "\n",
        "def fast_pg_insert(df: pd.DataFrame, connection: str, table_name: str, columns: List[str]) -> None:\n",
        "    \"\"\"\n",
        "        Inserts data from a pandas DataFrame into a PostgreSQL table using the COPY command for fast insertion.\n",
        "\n",
        "        Parameters:\n",
        "        df (pd.DataFrame): The DataFrame containing the data to be inserted.\n",
        "        connection (str): The connection string to the PostgreSQL database.\n",
        "        table_name (str): The name of the target table in the PostgreSQL database.\n",
        "        columns (List[str]): A list of column names in the target table that correspond to the DataFrame columns.\n",
        "\n",
        "        Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    conn = psycopg2.connect(connection)\n",
        "    _buffer = io.StringIO()\n",
        "    df.to_csv(_buffer, sep=\";\", index=False, header=False)\n",
        "    _buffer.seek(0)\n",
        "    with conn.cursor() as c:\n",
        "        c.copy_from(\n",
        "            file=_buffer,\n",
        "            table=table_name,\n",
        "            sep=\";\",\n",
        "            columns=columns,\n",
        "            null=''\n",
        "        )\n",
        "    conn.commit()\n",
        "    conn.close()"
      ],
      "metadata": {
        "id": "wZDxdvoP4Fov"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Database Schema\n",
        "We will create a database with two tables: podcast and segment:\n",
        "\n",
        "**podcast**\n",
        "\n",
        "- PK: id\n",
        " - The unique podcast id found in the huggingface data (i,e., TRdL6ZzWBS0  is the ID for Jed Buchwald: Isaac Newton and the Philosophy of Science | Lex Fridman Podcast #214)\n",
        "- title\n",
        " - The title of podcast (ie., Jed Buchwald: Isaac Newton and the Philosophy of Science | Lex Fridman Podcast #214)\n",
        "\n",
        "**segment**\n",
        "\n",
        "- PK: id\n",
        " - the unique identifier for the podcast segment. This was created by concatenating the podcast idx and the segment index together (ie., \"0;1\") is the 0th podcast and the 1st segment\n",
        "This is present in the as the \"custom_id\" field in the `embedding.jsonl` and batch_request.jsonl files\n",
        "- start_time\n",
        " - The start timestamp of the segment\n",
        "- end_time\n",
        " - The end timestamp of the segment\n",
        "- content\n",
        " - The raw text transcription of the podcast\n",
        "- embedding\n",
        " - the 128 dimensional vector representation of the text\n",
        "- FK: podcast_id\n",
        " - foreign key to podcast.id"
      ],
      "metadata": {
        "id": "7Y2HkhMZmHFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample document:\n",
        "# {\n",
        "#   \"custom_id\": \"89:115\",\n",
        "#   \"url\": \"/v1/embeddings\",\n",
        "#   \"method\": \"POST\",\n",
        "#   \"body\": {\n",
        "#     \"input\": \" have been possible without these approaches?\",\n",
        "#     \"model\": \"text-embedding-3-large\",\n",
        "#     \"dimensions\": 128,\n",
        "#     \"metadata\": {\n",
        "#       \"title\": \"Podcast: Boris Sofman: Waymo, Cozmo, Self-Driving Cars, and the Future of Robotics | Lex Fridman Podcast #241\",\n",
        "#       \"podcast_id\": \"U_AREIyd0Fc\",\n",
        "#       \"start_time\": 484.52,\n",
        "#       \"stop_time\": 487.08\n",
        "#     }\n",
        "#   }\n",
        "# }\n",
        "\n",
        "# Sample embedding:\n",
        "# {\n",
        "#   \"id\": \"batch_req_QZBmHS7FBiVABxcsGiDx2THJ\",\n",
        "#   \"custom_id\": \"89:115\",\n",
        "#   \"response\": {\n",
        "#     \"status_code\": 200,\n",
        "#     \"request_id\": \"7a55eba082c70aca9e7872d2b694f095\",\n",
        "#     \"body\": {\n",
        "#       \"object\": \"list\",\n",
        "#       \"data\": [\n",
        "#         {\n",
        "#           \"object\": \"embedding\",\n",
        "#           \"index\": 0,\n",
        "#           \"embedding\": [\n",
        "#             0.0035960325,\n",
        "#             126 more lines....\n",
        "#             -0.093248844\n",
        "#           ]\n",
        "#         }\n",
        "#       ],\n",
        "#       \"model\": \"text-embedding-3-large\",\n",
        "#       \"usage\": {\n",
        "#         \"prompt_tokens\": 7,\n",
        "#         \"total_tokens\": 7\n",
        "#       }\n",
        "#     }\n",
        "#   },\n",
        "#   \"error\": null\n",
        "# }"
      ],
      "metadata": {
        "id": "3EZuFdc9m9uP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create table statements that you'll write\n",
        "#TODO\n",
        "\n",
        "\n",
        "CREATE_EXTENSION = \"CREATE EXTENSION IF NOT EXISTS vector;\"\n",
        "\n",
        "CREATE_PODCAST_TABLE = \"\"\"\n",
        "CREATE TABLE IF NOT EXISTS podcast (\n",
        "    id TEXT PRIMARY KEY,\n",
        "    title TEXT NOT NULL\n",
        ");\n",
        "\"\"\"\n",
        "\n",
        "CREATE_SEGMENT_TABLE = \"\"\"\n",
        "CREATE TABLE IF NOT EXISTS segment (\n",
        "    id TEXT PRIMARY KEY,\n",
        "    podcast_id TEXT REFERENCES podcast(id),\n",
        "    start_time DOUBLE PRECISION,\n",
        "    end_time DOUBLE PRECISION,\n",
        "    content TEXT,\n",
        "    embedding VECTOR(128)\n",
        ");\n",
        "\"\"\"\n",
        "\n",
        "# connect and create tables\n",
        "conn = psycopg2.connect(CONNECTION)\n",
        "cur = conn.cursor()\n",
        "\n",
        "cur.execute(CREATE_EXTENSION)\n",
        "cur.execute(CREATE_PODCAST_TABLE)\n",
        "cur.execute(CREATE_SEGMENT_TABLE)\n",
        "\n",
        "conn.commit()\n",
        "cur.close()\n",
        "conn.close()\n"
      ],
      "metadata": {
        "id": "bU6fFAwb5EYO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Extract needed data out of JSONL files. This may be the hard part!\n",
        "\n",
        "# TODO: What data do we need?\n",
        "# TODO: What data is in the documents jsonl files?\n",
        "# TODO: What data is in the embedding jsonl files?\n",
        "# TODO: Get some pandas data frames for our two tables so we can copy the data in!\n",
        "\n",
        "import json\n",
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "def load_jsonl_files(path_pattern):\n",
        "    \"\"\"Loads and concatenates all JSONL files into a list of dicts.\"\"\"\n",
        "    data = []\n",
        "    for file_path in glob.glob(path_pattern):\n",
        "        with open(file_path, 'r') as f:\n",
        "            for line in f:\n",
        "                data.append(json.loads(line))\n",
        "    return data\n",
        "\n",
        "\n",
        "\n",
        "document_files = \"documents/documents/*.jsonl\"\n",
        "embedding_files = \"embedding/embedding/*.jsonl\"\n",
        "\n",
        "\n",
        "documents = load_jsonl_files(document_files)\n",
        "embeddings = load_jsonl_files(embedding_files)\n",
        "\n",
        "\n",
        "podcast_records = {}\n",
        "for d in documents:\n",
        "    meta = d[\"body\"][\"metadata\"]\n",
        "    podcast_id = meta[\"podcast_id\"]\n",
        "    title = meta[\"title\"]\n",
        "    if podcast_id not in podcast_records:\n",
        "        podcast_records[podcast_id] = title\n",
        "\n",
        "podcast_df = pd.DataFrame(list(podcast_records.items()), columns=[\"id\", \"title\"])\n",
        "\n",
        "segment_data = []\n",
        "embedding_map = {e[\"custom_id\"]: e[\"response\"][\"body\"][\"data\"][0][\"embedding\"]\n",
        "                 for e in embeddings if e.get(\"response\") and e[\"response\"].get(\"body\")}\n",
        "\n",
        "for d in documents:\n",
        "    meta = d[\"body\"][\"metadata\"]\n",
        "    seg_id = d[\"custom_id\"]\n",
        "    start = meta[\"start_time\"]\n",
        "    end = meta[\"stop_time\"]\n",
        "    content = d[\"body\"][\"input\"]\n",
        "    podcast_id = meta[\"podcast_id\"]\n",
        "    embedding = embedding_map.get(seg_id)\n",
        "    if embedding is not None:\n",
        "        segment_data.append({\n",
        "            \"id\": seg_id,\n",
        "            \"podcast_id\": podcast_id,\n",
        "            \"start_time\": start,\n",
        "            \"end_time\": end,\n",
        "            \"content\": content,\n",
        "            \"embedding\": embedding\n",
        "        })\n",
        "\n",
        "segment_df = pd.DataFrame(segment_data)"
      ],
      "metadata": {
        "id": "v81052OY5BKW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Optional #####\n",
        "# In addition to the embedding and document files you might like to load\n",
        "# the full podcast raw data via the hugging face datasets library\n",
        "\n",
        "# from datasets import load_dataset\n",
        "# ds = load_dataset(\"Whispering-GPT/lex-fridman-podcast\")\n"
      ],
      "metadata": {
        "id": "xo3Y8IHYRruE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO Copy all the \"podcast\" data into the podcast postgres table!\n",
        "fast_pg_insert(podcast_df, CONNECTION, \"podcast\", [\"id\", \"title\"])\n"
      ],
      "metadata": {
        "id": "5W3f-2iTpGL0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO Copy all the \"segment\" data into the segment postgres table!\n",
        "# HINT 1: use the recommender.utils.fast_pg_insert function to insert data into the database\n",
        "# otherwise inserting the 800k documents will take a very, very long time\n",
        "# HINT 2: if you don't want to use all your memory and crash\n",
        "# colab, you'll need to either send the data up in chunks\n",
        "# or write your own function for copying it up. Alternative to chunking maybe start\n",
        "# with writing it to a CSV and then copy it up?\n",
        "import numpy as np\n",
        "\n",
        "chunk_size = 50000  # adjust depending on your memory and DB speed\n",
        "num_rows = len(segment_df)\n",
        "\n",
        "for start in range(0, num_rows, chunk_size):\n",
        "    end = min(start + chunk_size, num_rows)\n",
        "    chunk = segment_df.iloc[start:end]\n",
        "    fast_pg_insert(chunk, CONNECTION, \"segment\",\n",
        "                   [\"id\", \"podcast_id\", \"start_time\", \"end_time\", \"content\", \"embedding\"])\n",
        "    print(f\"Inserted rows {start}–{end}\")"
      ],
      "metadata": {
        "id": "ZTUsciGfpahF",
        "outputId": "597ddc97-1f8f-4d4e-c503-1b81c220d2c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inserted rows 0–50000\n",
            "Inserted rows 50000–100000\n",
            "Inserted rows 100000–150000\n",
            "Inserted rows 150000–200000\n",
            "Inserted rows 200000–250000\n",
            "Inserted rows 250000–300000\n",
            "Inserted rows 300000–350000\n",
            "Inserted rows 350000–400000\n",
            "Inserted rows 400000–450000\n",
            "Inserted rows 450000–500000\n",
            "Inserted rows 500000–550000\n",
            "Inserted rows 550000–600000\n",
            "Inserted rows 600000–650000\n",
            "Inserted rows 650000–700000\n",
            "Inserted rows 700000–750000\n",
            "Inserted rows 750000–800000\n",
            "Inserted rows 800000–832839\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#This is a cell to check to make sure there are actually rows in the databases.\n",
        "\n",
        "import psycopg2\n",
        "\n",
        "conn = psycopg2.connect(CONNECTION)\n",
        "cur = conn.cursor()\n",
        "\n",
        "cur.execute(\"SELECT COUNT(*) FROM podcast;\")\n",
        "podcast_count = cur.fetchone()[0]\n",
        "\n",
        "cur.execute(\"SELECT COUNT(*) FROM segment;\")\n",
        "segment_count = cur.fetchone()[0]\n",
        "\n",
        "cur.close()\n",
        "conn.close()\n",
        "\n",
        "print(f\"podcast rows: {podcast_count}\")\n",
        "print(f\"segment rows: {segment_count}\")\n"
      ],
      "metadata": {
        "id": "NuMHm9gL1dvw",
        "outputId": "1493bfa7-230b-42da-e2ff-e7785d1ad0e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "podcast rows: 346\n",
            "segment rows: 832839\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## This script is used to query the database\n",
        "import os\n",
        "import psycopg2\n",
        "\n",
        "\n",
        "# Write your queries\n",
        "# Q1) What are the five most similar segments to segment \"267:476\"\n",
        "# Input: \"that if we were to meet alien life at some point\"\n",
        "# For each result return the podcast name, the segment id, segment raw text,  the start time, stop time, and embedding distance\n",
        "\n",
        "conn = psycopg2.connect(CONNECTION)\n",
        "cur = conn.cursor()\n",
        "cur.execute(\"\"\"\n",
        "\n",
        "\"\"\")\n",
        "for row in cur.fetchall():\n",
        "  print(row)\n",
        "\n",
        "conn.commit()\n",
        "conn.close()"
      ],
      "metadata": {
        "id": "NvkG-51G5IDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q2) What are the five most dissimilar segments to segment \"267:476\"\n",
        "# Input: \"that if we were to meet alien life at some point\"\n",
        "# For each result return the podcast name, the segment id, segment raw text, the start time, stop time, and embedding distance\n"
      ],
      "metadata": {
        "id": "Dq8ePSfrw8Ix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q3) What are the five most similar segments to segment '48:511'\n",
        "\n",
        "# Input: \"Is it is there something especially interesting and profound to you in terms of our current deep learning neural network, artificial neural network approaches and the whatever we do understand about the biological neural network.\"\n",
        "# For each result return the podcast name, the segment id, segment raw text,  the start time, stop time, and embedding distance\n"
      ],
      "metadata": {
        "id": "dmTK02bZk3pF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q4) What are the five most similar segments to segment '51:56'\n",
        "\n",
        "# Input: \"But what about like the fundamental physics of dark energy? Is there any understanding of what the heck it is?\"\n",
        "# For each result return the podcast name, the segment id, segment raw text,  the start time, stop time, and embedding distance\n"
      ],
      "metadata": {
        "id": "jcfhAKKQk9rV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q5) For each of the following podcast segments, find the five most similar podcast episodes. Hint: You can do this by averaging over the embedding vectors within a podcast episode.\n",
        "\n",
        "#     a) Segment \"267:476\"\n",
        "\n",
        "#     b) Segment '48:511'\n",
        "\n",
        "#     c) Segment '51:56'\n",
        "\n",
        "# For each result return the Podcast title and the embedding distance\n"
      ],
      "metadata": {
        "id": "OT4yTTn4k_iX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q6) For podcast episode id = VeH7qKZr0WI, find the five most similar podcast episodes. Hint: you can do a similar averaging procedure as Q5\n",
        "\n",
        "# Input Episode: \"Balaji Srinivasan: How to Fix Government, Twitter, Science, and the FDA | Lex Fridman Podcast #331\"\n",
        "# For each result return the Podcast title and the embedding distance\n"
      ],
      "metadata": {
        "id": "_oKIVjn4lBYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deliverables\n",
        "You will turn in a ZIP or PDF file containing all your code and a PDF file with the queries and results for questions 1-7."
      ],
      "metadata": {
        "id": "WBZVtZP4lDO2"
      }
    }
  ]
}