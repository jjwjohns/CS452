{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jjwjohns/CS452/blob/main/embed/VectorDB_Lab_CS452_(starter).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4UOUNsUTsvcn"
      },
      "outputs": [],
      "source": [
        "# Download datasets from kaggle\n",
        "\n",
        "import json\n",
        "import os\n",
        "\n",
        "if not os.path.exists(\"lex-fridman-text-embedding-3-large-128.zip\"):\n",
        "  kaggle_json = {\"username\": \"michaeltreynolds\",\"key\": \"149701be742f30a8a0526762c61beea0\"}\n",
        "  kaggle_dir = os.path.join(os.path.expanduser(\"~\"), \".kaggle\")\n",
        "  os.makedirs(kaggle_dir, exist_ok=True)\n",
        "  kaggle_config_path = os.path.join(kaggle_dir, \"kaggle.json\")\n",
        "  with open(kaggle_config_path, 'w') as f:\n",
        "    json.dump(kaggle_json, f)\n",
        "\n",
        "  !kaggle datasets download -d michaeltreynolds/lex-fridman-text-embedding-3-large-128\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip kaggle data\n",
        "\n",
        "!unzip lex-fridman-text-embedding-3-large-128.zip\n",
        "!unzip lex-fridman-text-embedding-3-large-128/*.zip\n"
      ],
      "metadata": {
        "id": "h3swnD70x4FG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2df8d65-8801-4afb-990a-d77276526abf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  lex-fridman-text-embedding-3-large-128.zip\n",
            "replace documents/documents/batch_request_0lw3vrQqdWbdBRurTGNMHU76.jsonl? [y]es, [n]o, [A]ll, [N]one, [r]ename: unzip:  cannot find or open lex-fridman-text-embedding-3-large-128/*.zip, lex-fridman-text-embedding-3-large-128/*.zip.zip or lex-fridman-text-embedding-3-large-128/*.zip.ZIP.\n",
            "\n",
            "No zipfiles found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use specific libraries\n",
        "!pip install datasets==2.20.0 psycopg2==2.9.9 pgcopy==1.6.0\n",
        "import psycopg2"
      ],
      "metadata": {
        "id": "SYDFzWfv4HLW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8c18525-8ca1-484a-ac3b-d2c2f1552181"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets==2.20.0 in /usr/local/lib/python3.12/dist-packages (2.20.0)\n",
            "Requirement already satisfied: psycopg2==2.9.9 in /usr/local/lib/python3.12/dist-packages (2.9.9)\n",
            "Requirement already satisfied: pgcopy==1.6.0 in /usr/local/lib/python3.12/dist-packages (1.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets==2.20.0) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets==2.20.0) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets==2.20.0) (18.1.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.12/dist-packages (from datasets==2.20.0) (0.7)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets==2.20.0) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets==2.20.0) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets==2.20.0) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets==2.20.0) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets==2.20.0) (3.6.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from datasets==2.20.0) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets==2.20.0) (2024.5.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from datasets==2.20.0) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.12/dist-packages (from datasets==2.20.0) (0.35.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets==2.20.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets==2.20.0) (6.0.3)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.12/dist-packages (from pgcopy==1.6.0) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.20.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.20.0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.20.0) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.20.0) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.20.0) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.20.0) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.20.0) (1.22.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.21.2->datasets==2.20.0) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.21.2->datasets==2.20.0) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==2.20.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==2.20.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==2.20.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==2.20.0) (2025.10.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==2.20.0) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==2.20.0) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==2.20.0) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get your own trial account at timescaledb and paste your own connection string\n",
        "\n",
        "#TODO\n",
        "CONNECTION = \"postgres://tsdbadmin:aqtd6xjf37fdg2fw@avhfgf31h1.veboosbd04.tsdb.cloud.timescale.com:35070/tsdb?sslmode=require\""
      ],
      "metadata": {
        "id": "ukT4dY-z25XG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use this if you want to start over on your postgres table!\n",
        "\n",
        "DROP_TABLE = \"DROP TABLE IF EXISTS podcast, segment\"\n",
        "with psycopg2.connect(CONNECTION) as conn:\n",
        "    cursor = conn.cursor()\n",
        "    cursor.execute(DROP_TABLE)\n",
        "    conn.commit() # Commit the changes\n"
      ],
      "metadata": {
        "id": "gpp_3EuU3SN-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Useful function that takes a pd.DataFrame and copies it directly into a table.\n",
        "\n",
        "import pandas as pd\n",
        "import io\n",
        "import psycopg2\n",
        "\n",
        "from typing import List\n",
        "\n",
        "def fast_pg_insert(df: pd.DataFrame, connection: str, table_name: str, columns: List[str]) -> None:\n",
        "    \"\"\"\n",
        "        Inserts data from a pandas DataFrame into a PostgreSQL table using the COPY command for fast insertion.\n",
        "\n",
        "        Parameters:\n",
        "        df (pd.DataFrame): The DataFrame containing the data to be inserted.\n",
        "        connection (str): The connection string to the PostgreSQL database.\n",
        "        table_name (str): The name of the target table in the PostgreSQL database.\n",
        "        columns (List[str]): A list of column names in the target table that correspond to the DataFrame columns.\n",
        "\n",
        "        Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    conn = psycopg2.connect(connection)\n",
        "    _buffer = io.StringIO()\n",
        "    df.to_csv(_buffer, sep=\";\", index=False, header=False)\n",
        "    _buffer.seek(0)\n",
        "    with conn.cursor() as c:\n",
        "        c.copy_from(\n",
        "            file=_buffer,\n",
        "            table=table_name,\n",
        "            sep=\";\",\n",
        "            columns=columns,\n",
        "            null=''\n",
        "        )\n",
        "    conn.commit()\n",
        "    conn.close()"
      ],
      "metadata": {
        "id": "wZDxdvoP4Fov"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Database Schema\n",
        "We will create a database with two tables: podcast and segment:\n",
        "\n",
        "**podcast**\n",
        "\n",
        "- PK: id\n",
        " - The unique podcast id found in the huggingface data (i,e., TRdL6ZzWBS0  is the ID for Jed Buchwald: Isaac Newton and the Philosophy of Science | Lex Fridman Podcast #214)\n",
        "- title\n",
        " - The title of podcast (ie., Jed Buchwald: Isaac Newton and the Philosophy of Science | Lex Fridman Podcast #214)\n",
        "\n",
        "**segment**\n",
        "\n",
        "- PK: id\n",
        " - the unique identifier for the podcast segment. This was created by concatenating the podcast idx and the segment index together (ie., \"0;1\") is the 0th podcast and the 1st segment\n",
        "This is present in the as the \"custom_id\" field in the `embedding.jsonl` and batch_request.jsonl files\n",
        "- start_time\n",
        " - The start timestamp of the segment\n",
        "- end_time\n",
        " - The end timestamp of the segment\n",
        "- content\n",
        " - The raw text transcription of the podcast\n",
        "- embedding\n",
        " - the 128 dimensional vector representation of the text\n",
        "- FK: podcast_id\n",
        " - foreign key to podcast.id"
      ],
      "metadata": {
        "id": "7Y2HkhMZmHFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample document:\n",
        "# {\n",
        "#   \"custom_id\": \"89:115\",\n",
        "#   \"url\": \"/v1/embeddings\",\n",
        "#   \"method\": \"POST\",\n",
        "#   \"body\": {\n",
        "#     \"input\": \" have been possible without these approaches?\",\n",
        "#     \"model\": \"text-embedding-3-large\",\n",
        "#     \"dimensions\": 128,\n",
        "#     \"metadata\": {\n",
        "#       \"title\": \"Podcast: Boris Sofman: Waymo, Cozmo, Self-Driving Cars, and the Future of Robotics | Lex Fridman Podcast #241\",\n",
        "#       \"podcast_id\": \"U_AREIyd0Fc\",\n",
        "#       \"start_time\": 484.52,\n",
        "#       \"stop_time\": 487.08\n",
        "#     }\n",
        "#   }\n",
        "# }\n",
        "\n",
        "# Sample embedding:\n",
        "# {\n",
        "#   \"id\": \"batch_req_QZBmHS7FBiVABxcsGiDx2THJ\",\n",
        "#   \"custom_id\": \"89:115\",\n",
        "#   \"response\": {\n",
        "#     \"status_code\": 200,\n",
        "#     \"request_id\": \"7a55eba082c70aca9e7872d2b694f095\",\n",
        "#     \"body\": {\n",
        "#       \"object\": \"list\",\n",
        "#       \"data\": [\n",
        "#         {\n",
        "#           \"object\": \"embedding\",\n",
        "#           \"index\": 0,\n",
        "#           \"embedding\": [\n",
        "#             0.0035960325,\n",
        "#             126 more lines....\n",
        "#             -0.093248844\n",
        "#           ]\n",
        "#         }\n",
        "#       ],\n",
        "#       \"model\": \"text-embedding-3-large\",\n",
        "#       \"usage\": {\n",
        "#         \"prompt_tokens\": 7,\n",
        "#         \"total_tokens\": 7\n",
        "#       }\n",
        "#     }\n",
        "#   },\n",
        "#   \"error\": null\n",
        "# }"
      ],
      "metadata": {
        "id": "3EZuFdc9m9uP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create table statements that you'll write\n",
        "#TODO\n",
        "\n",
        "\n",
        "CREATE_EXTENSION = \"CREATE EXTENSION IF NOT EXISTS vector;\"\n",
        "\n",
        "CREATE_PODCAST_TABLE = \"\"\"\n",
        "CREATE TABLE IF NOT EXISTS podcast (\n",
        "    id TEXT PRIMARY KEY,\n",
        "    title TEXT NOT NULL\n",
        ");\n",
        "\"\"\"\n",
        "\n",
        "CREATE_SEGMENT_TABLE = \"\"\"\n",
        "CREATE TABLE IF NOT EXISTS segment (\n",
        "    id TEXT PRIMARY KEY,\n",
        "    podcast_id TEXT REFERENCES podcast(id),\n",
        "    start_time DOUBLE PRECISION,\n",
        "    end_time DOUBLE PRECISION,\n",
        "    content TEXT,\n",
        "    embedding VECTOR(128)\n",
        ");\n",
        "\"\"\"\n",
        "\n",
        "# connect and create tables\n",
        "conn = psycopg2.connect(CONNECTION)\n",
        "cur = conn.cursor()\n",
        "\n",
        "cur.execute(CREATE_EXTENSION)\n",
        "cur.execute(CREATE_PODCAST_TABLE)\n",
        "cur.execute(CREATE_SEGMENT_TABLE)\n",
        "\n",
        "conn.commit()\n",
        "cur.close()\n",
        "conn.close()\n"
      ],
      "metadata": {
        "id": "bU6fFAwb5EYO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Extract needed data out of JSONL files. This may be the hard part!\n",
        "\n",
        "# TODO: What data do we need?\n",
        "# TODO: What data is in the documents jsonl files?\n",
        "# TODO: What data is in the embedding jsonl files?\n",
        "# TODO: Get some pandas data frames for our two tables so we can copy the data in!\n",
        "\n",
        "import json\n",
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "def load_jsonl_files(path_pattern):\n",
        "    \"\"\"Loads and concatenates all JSONL files into a list of dicts.\"\"\"\n",
        "    data = []\n",
        "    for file_path in glob.glob(path_pattern):\n",
        "        with open(file_path, 'r') as f:\n",
        "            for line in f:\n",
        "                data.append(json.loads(line))\n",
        "    return data\n",
        "\n",
        "\n",
        "\n",
        "document_files = \"documents/documents/*.jsonl\"\n",
        "embedding_files = \"embedding/embedding/*.jsonl\"\n",
        "\n",
        "\n",
        "documents = load_jsonl_files(document_files)\n",
        "embeddings = load_jsonl_files(embedding_files)\n",
        "\n",
        "\n",
        "podcast_records = {}\n",
        "for d in documents:\n",
        "    meta = d[\"body\"][\"metadata\"]\n",
        "    podcast_id = meta[\"podcast_id\"]\n",
        "    title = meta[\"title\"]\n",
        "    if podcast_id not in podcast_records:\n",
        "        podcast_records[podcast_id] = title\n",
        "\n",
        "podcast_df = pd.DataFrame(list(podcast_records.items()), columns=[\"id\", \"title\"])\n",
        "\n",
        "segment_data = []\n",
        "embedding_map = {e[\"custom_id\"]: e[\"response\"][\"body\"][\"data\"][0][\"embedding\"]\n",
        "                 for e in embeddings if e.get(\"response\") and e[\"response\"].get(\"body\")}\n",
        "\n",
        "for d in documents:\n",
        "    meta = d[\"body\"][\"metadata\"]\n",
        "    seg_id = d[\"custom_id\"]\n",
        "    start = meta[\"start_time\"]\n",
        "    end = meta[\"stop_time\"]\n",
        "    content = d[\"body\"][\"input\"]\n",
        "    podcast_id = meta[\"podcast_id\"]\n",
        "    embedding = embedding_map.get(seg_id)\n",
        "    if embedding is not None:\n",
        "        segment_data.append({\n",
        "            \"id\": seg_id,\n",
        "            \"podcast_id\": podcast_id,\n",
        "            \"start_time\": start,\n",
        "            \"end_time\": end,\n",
        "            \"content\": content,\n",
        "            \"embedding\": embedding\n",
        "        })\n",
        "\n",
        "segment_df = pd.DataFrame(segment_data)"
      ],
      "metadata": {
        "id": "v81052OY5BKW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Optional #####\n",
        "# In addition to the embedding and document files you might like to load\n",
        "# the full podcast raw data via the hugging face datasets library\n",
        "\n",
        "# from datasets import load_dataset\n",
        "# ds = load_dataset(\"Whispering-GPT/lex-fridman-podcast\")\n"
      ],
      "metadata": {
        "id": "xo3Y8IHYRruE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO Copy all the \"podcast\" data into the podcast postgres table!\n",
        "fast_pg_insert(podcast_df, CONNECTION, \"podcast\", [\"id\", \"title\"])\n"
      ],
      "metadata": {
        "id": "5W3f-2iTpGL0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO Copy all the \"segment\" data into the segment postgres table!\n",
        "# HINT 1: use the recommender.utils.fast_pg_insert function to insert data into the database\n",
        "# otherwise inserting the 800k documents will take a very, very long time\n",
        "# HINT 2: if you don't want to use all your memory and crash\n",
        "# colab, you'll need to either send the data up in chunks\n",
        "# or write your own function for copying it up. Alternative to chunking maybe start\n",
        "# with writing it to a CSV and then copy it up?\n",
        "import numpy as np\n",
        "\n",
        "chunk_size = 50000  # adjust depending on your memory and DB speed\n",
        "num_rows = len(segment_df)\n",
        "\n",
        "for start in range(0, num_rows, chunk_size):\n",
        "    end = min(start + chunk_size, num_rows)\n",
        "    chunk = segment_df.iloc[start:end]\n",
        "    fast_pg_insert(chunk, CONNECTION, \"segment\",\n",
        "                   [\"id\", \"podcast_id\", \"start_time\", \"end_time\", \"content\", \"embedding\"])\n",
        "    print(f\"Inserted rows {start}–{end}\")"
      ],
      "metadata": {
        "id": "ZTUsciGfpahF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "597ddc97-1f8f-4d4e-c503-1b81c220d2c2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inserted rows 0–50000\n",
            "Inserted rows 50000–100000\n",
            "Inserted rows 100000–150000\n",
            "Inserted rows 150000–200000\n",
            "Inserted rows 200000–250000\n",
            "Inserted rows 250000–300000\n",
            "Inserted rows 300000–350000\n",
            "Inserted rows 350000–400000\n",
            "Inserted rows 400000–450000\n",
            "Inserted rows 450000–500000\n",
            "Inserted rows 500000–550000\n",
            "Inserted rows 550000–600000\n",
            "Inserted rows 600000–650000\n",
            "Inserted rows 650000–700000\n",
            "Inserted rows 700000–750000\n",
            "Inserted rows 750000–800000\n",
            "Inserted rows 800000–832839\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#This is a cell to check to make sure there are actually rows in the databases.\n",
        "\n",
        "import psycopg2\n",
        "\n",
        "conn = psycopg2.connect(CONNECTION)\n",
        "cur = conn.cursor()\n",
        "\n",
        "cur.execute(\"SELECT COUNT(*) FROM podcast;\")\n",
        "podcast_count = cur.fetchone()[0]\n",
        "\n",
        "cur.execute(\"SELECT COUNT(*) FROM segment;\")\n",
        "segment_count = cur.fetchone()[0]\n",
        "\n",
        "cur.close()\n",
        "conn.close()\n",
        "\n",
        "print(f\"podcast rows: {podcast_count}\")\n",
        "print(f\"segment rows: {segment_count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NuMHm9gL1dvw",
        "outputId": "1493bfa7-230b-42da-e2ff-e7785d1ad0e6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "podcast rows: 346\n",
            "segment rows: 832839\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## This script is used to query the database\n",
        "import os\n",
        "import psycopg2\n",
        "\n",
        "\n",
        "# Write your queries\n",
        "# Q1) What are the five most similar segments to segment \"267:476\"\n",
        "# Input: \"that if we were to meet alien life at some point\"\n",
        "# For each result return the podcast name, the segment id, segment raw text,  the start time, stop time, and embedding distance\n",
        "\n",
        "conn = psycopg2.connect(CONNECTION)\n",
        "cur = conn.cursor()\n",
        "cur.execute(\"\"\"\n",
        "  SELECT\n",
        "      p.title AS podcast_title,\n",
        "      s.id AS segment_id,\n",
        "      s.content AS segment_text,\n",
        "      s.start_time,\n",
        "      s.end_time,\n",
        "      s.embedding <=> target.embedding AS distance\n",
        "  FROM segment s\n",
        "  JOIN podcast p ON s.podcast_id = p.id,\n",
        "      segment target\n",
        "  WHERE target.id = '267:476'\n",
        "    AND s.id <> target.id\n",
        "  ORDER BY s.embedding <=> target.embedding\n",
        "  LIMIT 5;\n",
        "  \"\"\"\n",
        "  )\n",
        "for row in cur.fetchall():\n",
        "  print(row)\n",
        "\n",
        "conn.commit()\n",
        "conn.close()"
      ],
      "metadata": {
        "id": "NvkG-51G5IDe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca05962b-42d3-495d-b3f1-04e37448bc2d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Podcast: Ryan Graves: UFOs, Fighter Jets, and Aliens | Lex Fridman Podcast #308', '113:2792', ' encounters, human beings, if we were to meet another alien', 6725.62, 6729.86, 0.21017563343048096)\n",
            "('Podcast: Richard Dawkins: Evolution, Intelligence, Simulation, and Memes | Lex Fridman Podcast #87', '268:1019', ' Suppose we did meet an alien from outer space', 2900.04, 2903.0800000000004, 0.215043842792511)\n",
            "('Podcast: Jeffrey Shainline: Neuromorphic Computing and Optoelectronic Intelligence | Lex Fridman Podcast #225', '305:3600', ' but if we think of alien civilizations out there', 9479.960000000001, 9484.04, 0.21749870672829563)\n",
            "('Podcast: Michio Kaku: Future of Humans, Aliens, Space Travel & Physics | Lex Fridman Podcast #45', '18:464', ' So I think when we meet alien life from outer space,', 1316.8600000000001, 1319.5800000000002, 0.22191289728620311)\n",
            "('Podcast: Alien Debate: Sara Walker and Lee Cronin | Lex Fridman Podcast #279', '71:989', ' because if aliens come to us', 2342.34, 2343.6200000000003, 0.22733632407594662)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q2) What are the five most dissimilar segments to segment \"267:476\"\n",
        "# Input: \"that if we were to meet alien life at some point\"\n",
        "# For each result return the podcast name, the segment id, segment raw text, the start time, stop time, and embedding distance\n",
        "conn = psycopg2.connect(CONNECTION)\n",
        "cur = conn.cursor()\n",
        "cur.execute(\"\"\"\n",
        "  SELECT\n",
        "      p.title AS podcast_title,\n",
        "      s.id AS segment_id,\n",
        "      s.content AS segment_text,\n",
        "      s.start_time,\n",
        "      s.end_time,\n",
        "      s.embedding <=> target.embedding AS distance\n",
        "  FROM segment s\n",
        "  JOIN podcast p ON s.podcast_id = p.id,\n",
        "      segment target\n",
        "  WHERE target.id = '267:476'\n",
        "    AND s.id <> target.id\n",
        "  ORDER BY s.embedding <=> target.embedding DESC\n",
        "  LIMIT 5;\n",
        "  \"\"\"\n",
        "  )\n",
        "for row in cur.fetchall():\n",
        "  print(row)\n",
        "\n",
        "conn.commit()\n",
        "conn.close()"
      ],
      "metadata": {
        "id": "Dq8ePSfrw8Ix",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1bdfa9a-d4e3-45c7-c43b-dbe29ba5db61"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Podcast: Jason Calacanis: Startups, Angel Investing, Capitalism, and Friendship | Lex Fridman Podcast #161', '119:218', ' a 73 Mustang Grande in gold?', 519.96, 523.8000000000001, 1.3053543269634247)\n",
            "('Podcast: Rana el Kaliouby: Emotion AI, Social Robots, and Self-Driving Cars | Lex Fridman Podcast #322', '133:2006', ' for 94 car models.', 5818.62, 5820.82, 1.258230745792389)\n",
            "('Podcast: Travis Stevens: Judo, Olympics, and Mental Toughness | Lex Fridman Podcast #223', '283:1488', ' when I called down to get the sauna.', 3709.34, 3711.1000000000004, 1.2364609837532043)\n",
            "('Podcast: Jeremy Howard: fast.ai Deep Learning Courses and Research | Lex Fridman Podcast #35', '241:1436', ' which has all the courses pre-installed.', 4068.9, 4071.1400000000003, 1.226698084276801)\n",
            "('Podcast: Joscha Bach: Nature of Reality, Dreams, and Consciousness | Lex Fridman Podcast #212', '307:3933', ' and very few are first class and some are budget.', 10648.64, 10650.960000000001, 1.2193505465984344)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q3) What are the five most similar segments to segment '48:511'\n",
        "\n",
        "# Input: \"Is it is there something especially interesting and profound to you in terms of our current deep learning neural network, artificial neural network approaches and the whatever we do understand about the biological neural network.\"\n",
        "# For each result return the podcast name, the segment id, segment raw text,  the start time, stop time, and embedding distance\n",
        "\n",
        "conn = psycopg2.connect(CONNECTION)\n",
        "cur = conn.cursor()\n",
        "cur.execute(\"\"\"\n",
        "  SELECT\n",
        "      p.title AS podcast_title,\n",
        "      s.id AS segment_id,\n",
        "      s.content AS segment_text,\n",
        "      s.start_time,\n",
        "      s.end_time,\n",
        "      s.embedding <=> target.embedding AS distance\n",
        "  FROM segment s\n",
        "  JOIN podcast p ON s.podcast_id = p.id,\n",
        "      segment target\n",
        "  WHERE target.id = '48:511'\n",
        "    AND s.id <> target.id\n",
        "  ORDER BY s.embedding <=> target.embedding\n",
        "  LIMIT 5;\n",
        "  \"\"\"\n",
        "  )\n",
        "for row in cur.fetchall():\n",
        "  print(row)\n",
        "\n",
        "conn.commit()\n",
        "conn.close()\n"
      ],
      "metadata": {
        "id": "dmTK02bZk3pF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95394009-b2a9-4bb6-e9a7-9c9090291ccd"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Podcast: Andrew Huberman: Neuroscience of Optimal Performance | Lex Fridman Podcast #139', '155:648', ' Is there something interesting to you or fundamental to you about the circuitry of the brain', 3798.48, 3805.84, 0.2127474058198181)\n",
            "('Podcast: Cal Newport: Deep Work, Focus, Productivity, Email, and Social Media | Lex Fridman Podcast #166', '61:3707', ' of what we might discover about neural networks?', 8498.02, 8500.1, 0.25354678949992593)\n",
            "('Podcast: Matt Botvinick: Neuroscience, Psychology, and AI at DeepMind | Lex Fridman Podcast #106', '48:512', \" And our brain is there. There's some there's quite a few differences. Are some of them to you either interesting or perhaps profound in terms of in terms of the gap we might want to try to close in trying to create a human level intelligence.\", 1846.84, 1865.84, 0.2588835137749762)\n",
            "('Podcast: Yann LeCun: Dark Matter of Intelligence and Self-Supervised Learning | Lex Fridman Podcast #258', '276:2642', ' Have these, I mean, small pockets of beautiful complexity. Does that, do cellular automata, do these kinds of emergence and complex systems give you some intuition or guide your understanding of machine learning systems and neural networks and so on?', 8628.16, 8646.16, 0.2706432125164032)\n",
            "('Podcast: Stephen Wolfram: Fundamental Theory of Physics, Life, and the Universe | Lex Fridman Podcast #124', '2:152', ' So is there something like that with physics where so deep learning neural networks have been around for a long time?', 610.86, 618.86, 0.2713611666585055)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q4) What are the five most similar segments to segment '51:56'\n",
        "\n",
        "# Input: \"But what about like the fundamental physics of dark energy? Is there any understanding of what the heck it is?\"\n",
        "# For each result return the podcast name, the segment id, segment raw text,  the start time, stop time, and embedding distance\n",
        "\n",
        "conn = psycopg2.connect(CONNECTION)\n",
        "cur = conn.cursor()\n",
        "cur.execute(\"\"\"\n",
        "  SELECT\n",
        "      p.title AS podcast_title,\n",
        "      s.id AS segment_id,\n",
        "      s.content AS segment_text,\n",
        "      s.start_time,\n",
        "      s.end_time,\n",
        "      s.embedding <=> target.embedding AS distance\n",
        "  FROM segment s\n",
        "  JOIN podcast p ON s.podcast_id = p.id,\n",
        "      segment target\n",
        "  WHERE target.id = '51:56'\n",
        "    AND s.id <> target.id\n",
        "  ORDER BY s.embedding <=> target.embedding\n",
        "  LIMIT 5;\n",
        "  \"\"\"\n",
        "  )\n",
        "for row in cur.fetchall():\n",
        "  print(row)\n",
        "\n",
        "conn.commit()\n",
        "conn.close()\n"
      ],
      "metadata": {
        "id": "jcfhAKKQk9rV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ac313c8-3962-4ac3-dc08-a71bdd96faa0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Podcast: George Hotz: Hacking the Simulation & Learning to Drive with Neural Nets | Lex Fridman Podcast #132', '308:144', \" I mean, we don't understand dark energy, right?\", 500.44, 502.6, 0.2232432828400549)\n",
            "('Podcast: Lex Fridman: Ask Me Anything - AMA January 2021 | Lex Fridman Podcast', '243:273', \" Like, what's up with this dark matter and dark energy stuff?\", 946.22, 950.12, 0.27051763114227934)\n",
            "('Podcast: Katherine de Kleer: Planets, Moons, Asteroids & Life in Our Solar System | Lex Fridman Podcast #184', '196:685', ' being like, what the hell is dark matter and dark energy?', 2591.72, 2595.9599999999996, 0.29117163524965817)\n",
            "('Podcast: Alex Filippenko: Supernovae, Dark Energy, Aliens & the Expanding Universe | Lex Fridman Podcast #137', '51:36', ' Do we have any understanding of what the heck that thing is?', 216.0, 219.0, 0.3137918523674087)\n",
            "('Podcast: Leonard Susskind: Quantum Mechanics, String Theory and Black Holes | Lex Fridman Podcast #41', '122:831', ' That is a big question in physics right now.', 2374.9, 2377.6200000000003, 0.3218188690055078)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q5) For each of the following podcast segments, find the five most similar podcast episodes. Hint: You can do this by averaging over the embedding vectors within a podcast episode.\n",
        "\n",
        "#     a) Segment \"267:476\"\n",
        "\n",
        "#     b) Segment '48:511'\n",
        "\n",
        "#     c) Segment '51:56'\n",
        "\n",
        "# For each result return the Podcast title and the embedding distance\n",
        "\n",
        "import psycopg2\n",
        "\n",
        "conn = psycopg2.connect(CONNECTION)\n",
        "cur = conn.cursor()\n",
        "\n",
        "for seg_id in ['267:476', '48:511', '51:56']:\n",
        "\n",
        "    cur.execute(f\"\"\"\n",
        "    WITH episode_embeddings AS (\n",
        "        SELECT\n",
        "            podcast_id,\n",
        "            AVG(embedding) AS avg_embedding\n",
        "        FROM segment\n",
        "        GROUP BY podcast_id\n",
        "    ),\n",
        "    target_segment AS (\n",
        "        SELECT embedding\n",
        "        FROM segment\n",
        "        WHERE id = '{seg_id}'\n",
        "    )\n",
        "    SELECT\n",
        "        p.title AS podcast_title,\n",
        "        e.podcast_id,\n",
        "        e.avg_embedding <=> t.embedding AS distance\n",
        "    FROM episode_embeddings e\n",
        "    JOIN target_segment t ON TRUE\n",
        "    JOIN podcast p ON e.podcast_id = p.id\n",
        "    ORDER BY e.avg_embedding <=> t.embedding\n",
        "    LIMIT 5;\n",
        "    \"\"\")\n",
        "\n",
        "    for row in cur.fetchall():\n",
        "        print(row)\n",
        "    print()\n",
        "    print()\n",
        "\n",
        "cur.close()\n",
        "conn.close()\n"
      ],
      "metadata": {
        "id": "OT4yTTn4k_iX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "643b5ed5-c52f-4e20-ee10-f45aa9b40d16"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Podcast: Sara Walker: The Origin of Life on Earth and Alien Worlds | Lex Fridman Podcast #198', '-tDQ74I3Ovs', 0.3627852071991431)\n",
            "('Podcast: Max Tegmark: Life 3.0 | Lex Fridman Podcast #1', 'Gi8LUnhP5yU', 0.3665250196073555)\n",
            "('Podcast: Martin Rees: Black Holes, Alien Life, Dark Matter, and the Big Bang | Lex Fridman Podcast #305', '50r-5ULcWgY', 0.366617733487539)\n",
            "('Podcast: Sean Carroll: The Nature of the Universe, Life, and Intelligence | Lex Fridman Podcast #26', 'l-NJrvyRo0c', 0.36957322796166536)\n",
            "('Podcast: Avi Loeb: Aliens, Black Holes, and the Mystery of the Oumuamua | Lex Fridman Podcast #154', 'plcc6E-E1uU', 0.372014183656066)\n",
            "\n",
            "\n",
            "('Podcast: Matt Botvinick: Neuroscience, Psychology, and AI at DeepMind | Lex Fridman Podcast #106', '3t06ajvBtl0', 0.31663737088095023)\n",
            "('Podcast: Christof Koch: Consciousness | Lex Fridman Podcast #2', 'piHkfmeU7Wo', 0.32103303452125387)\n",
            "('Podcast: Tomaso Poggio: Brains, Minds, and Machines | Lex Fridman Podcast #13', 'aSyZvBrPAyk', 0.3266379613602618)\n",
            "('Podcast: Dileep George: Brain-Inspired AI | Lex Fridman Podcast #115', 'tg_m_LxxRwM', 0.3294945744697447)\n",
            "('Podcast: Elon Musk: Neuralink, AI, Autopilot, and the Pale Blue Dot | Lex Fridman Podcast #49', 'smK9dgdTl40', 0.34698832040375893)\n",
            "\n",
            "\n",
            "('Podcast: Sean Carroll: Quantum Mechanics and the Many-Worlds Interpretation | Lex Fridman Podcast #47', 'iNqqOLscOBY', 0.3626996957310278)\n",
            "('Podcast: Alex Filippenko: Supernovae, Dark Energy, Aliens & the Expanding Universe | Lex Fridman Podcast #137', 'WxfA1OSev4c', 0.3914249873242598)\n",
            "('Podcast: Stephen Wolfram: Fundamental Theory of Physics, Life, and the Universe | Lex Fridman Podcast #124', '-t1_ffaFXao', 0.409155548080291)\n",
            "('Podcast: Donald Hoffman: Reality is an Illusion - How Evolution Hid the Truth | Lex Fridman Podcast #293', 'reYdQYZ9Rj4', 0.41927730449234635)\n",
            "('Podcast: Cumrun Vafa: String Theory | Lex Fridman Podcast #204', 'j4_VyRDOmN4', 0.4211102335246454)\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q6) For podcast episode id = VeH7qKZr0WI, find the five most similar podcast episodes. Hint: you can do a similar averaging procedure as Q5\n",
        "\n",
        "# Input Episode: \"Balaji Srinivasan: How to Fix Government, Twitter, Science, and the FDA | Lex Fridman Podcast #331\"\n",
        "# For each result return the Podcast title and the embedding distance\n",
        "import psycopg2\n",
        "\n",
        "conn = psycopg2.connect(CONNECTION)\n",
        "cur = conn.cursor()\n",
        "\n",
        "cur.execute(\"\"\"\n",
        "WITH episode_embeddings AS (\n",
        "    SELECT\n",
        "        podcast_id,\n",
        "        AVG(embedding) AS avg_embedding\n",
        "    FROM segment\n",
        "    GROUP BY podcast_id\n",
        "),\n",
        "target_episode AS (\n",
        "    SELECT avg_embedding\n",
        "    FROM episode_embeddings\n",
        "    WHERE podcast_id = 'VeH7qKZr0WI'\n",
        ")\n",
        "SELECT\n",
        "    p.title AS podcast_title,\n",
        "    e.podcast_id,\n",
        "    e.avg_embedding <=> t.avg_embedding AS distance\n",
        "FROM episode_embeddings e\n",
        "JOIN target_episode t ON TRUE\n",
        "JOIN podcast p ON e.podcast_id = p.id\n",
        "WHERE e.podcast_id <> 'VeH7qKZr0WI'\n",
        "ORDER BY e.avg_embedding <=> t.avg_embedding\n",
        "LIMIT 5;\n",
        "\"\"\")\n",
        "\n",
        "for row in cur.fetchall():\n",
        "    print(row)\n",
        "\n",
        "cur.close()\n",
        "conn.close()"
      ],
      "metadata": {
        "id": "_oKIVjn4lBYD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a765cee-de14-4863-e6fb-af47cdf1421b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Podcast: Tyler Cowen: Economic Growth & the Fight Against Conformity & Mediocrity | Lex Fridman Podcast #174', '7Grseeycor4', 0.03526234580879306)\n",
            "('Podcast: Brian Armstrong: Coinbase, Cryptocurrency, and Government Regulation | Lex Fridman Podcast #307', 'VBPTFlpv31k', 0.03845173749878528)\n",
            "('Podcast: Eric Weinstein: Difficult Conversations, Freedom of Speech, and Physics | Lex Fridman Podcast #163', 'ifX_JnBfxTY', 0.03919897986351917)\n",
            "('Podcast: Michael Malice and Yaron Brook: Ayn Rand, Human Nature, and Anarchy | Lex Fridman Podcast #178', 'Pl3x4GINtBQ', 0.039379995348458596)\n",
            "('Podcast: Michael Malice: The White Pill, Freedom, Hope, and Happiness Amidst Chaos | Lex Fridman Podcast #150', 'uykM3NhJbso', 0.041297568230806436)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deliverables\n",
        "You will turn in a ZIP or PDF file containing all your code and a PDF file with the queries and results for questions 1-7."
      ],
      "metadata": {
        "id": "WBZVtZP4lDO2"
      }
    }
  ]
}